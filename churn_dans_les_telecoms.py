# -*- coding: utf-8 -*-
"""churn dans les telecoms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dPWWYV1PAK50x3ULkZuKss13iTsC8AaG

# Problématique
Le client est "Pinky" une entreprise de télécommunication, qui vend des forfaits téléphonique et internet. Ses clients sont donc abonnés, et paient mensuellement. Mais chaque mois, des clients résilient leur abonnement, c'est ce qu'on appelle le taux d'attrition (churn rate en anglais). C'est un indicateur très employé dans tous les secteurs avec des abonnements (télécom, énergie, banque, assurance, etc...).

Pinky dispose d'une plateforme d'appel, avec des conseillers. Les conseillers peuvent appeler les clients pour leur faire des propositions commerciales. Pinky a remarqué qu'il est presqu'impossible de faire revenir un client qui a résilié. Il préfère donc faire des propositions commerciales aux clients "à risque", avant qu'ils ne résilient.

C'est pourquoi Pinky fait appel à toi ! Pinky te fournit une extraction d'un échantillon représentatif de sa base client, dont des clients qui ont résilié ce mois-ci. Ton but va être de décrire les caractéristiques des clients ayant résilié et ce qui les distingue des autres clients. Puis tu devras proposer un scoring pour chaque client, afin de prioriser les appels vers les clients les plus à risque.

# Import
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, accuracy_score, roc_auc_score, classification_report, confusion_matrix, plot_confusion_matrix
from sklearn import metrics

df = pd.read_csv('https://raw.githubusercontent.com/murpi/wilddata/master/quests/churn_telecom.csv', sep = ',', skipinitialspace = True)
df.head(5)

df.info()

"""# Préparation des données

## Colonnes numériques
"""

df['SeniorCitizen'].value_counts()

df['tenure'].value_counts()

df['MonthlyCharges'].value_counts()

df['TotalCharges'] = df['TotalCharges'].astype(float)
df['TotalCharges'].value_counts()

"""## Encodage des colonnes catégorielles binaires"""

# Fonction qui prend en entré un dataframe et un nom de colonne
# et retourne la répartition des valeurs de la colonne
# ainsi que la factorisation des colonnes
def facto(df, colonne) :
    print('Répartition des valeurs de la colonne ' + colonne)
    print(end = '\n')
    print(df[colonne].value_counts(normalize = True))
    print(end = '\n')
    plt.title('Répartition des '+ colonne + ' selon le churn')
    sns.histplot(data = df, x = df[colonne], multiple="dodge", hue = df['Churn'], palette = 'Blues')
    plt.show()
    df[colonne] = df[colonne].factorize()[0]
    return

columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']

for i in columns :
    facto(df, i)

df.head(5)

"""## Encodage des colonnes catégorielles avec plusieurs classes sans ordre """

# Fonction qui prend en entrée un dataframe et un nom de colonne 
# et qui retourne la repartition des valeurs de la colonne
# ainsi que son graphique
def graphique(df, colonne) : 
    print('Répartition des valeurs de la colonne ' + colonne)
    print(end = '\n')
    print(df[colonne].value_counts(normalize = True))
    print(end = '\n')
    plt.title('Répartition des '+ colonne + ' selon le churn')
    sns.histplot(data = df, x = df[colonne], multiple="dodge", hue = df['Churn'], palette = 'Blues')
    plt.show()
    
    return

columns = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']

for i in columns :
    graphique(df, i)

# Get dummies sur les colonnes caégorielles sans ordre
df = pd.get_dummies(df, columns = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod'])

"""## Colonne cible Churn"""

# Taux d'attrition
df['Churn'].value_counts(normalize = True)

df.info()

df = df.dropna()

"""# Heatmap"""

# Heatmap qui montre les corrélations entre les colonnes
plt.figure(figsize = (13, 7))
sns.heatmap(df.corr(), cmap = 'vlag', center = 0);

"""# Machine Learning

## PCA
"""

# Définition du X et y
X = df.drop(columns = ['customerID', 'Churn'])
y = df[['Churn']]

# Standardisation
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)

# Initialisation du PCA et entrainement
pca = PCA(n_components=2)
pca.fit(X_scaled)

# Composantes principales expliquant la variance
pca.explained_variance_ratio_

X_pca = pca.fit_transform(X_scaled)
X_pca = pd.DataFrame(X_pca)
X_pca['Churn'] = df['Churn']
X_pca

sns.scatterplot(data = X_pca, x = X_pca.iloc[:,0], y = X_pca.iloc[:,1], hue = 'Churn', palette = "Paired_r")
plt.show()

"""## K-means"""

# Définition de X
X = df.drop(columns = ['customerID', 'Churn'])

# Standardisation
scaler = StandardScaler().fit(X)
X_scaled = pd.DataFrame(scaler.transform(X), columns=X.columns)

# Détermination du nombre optimal de cluster
for k in range(2,11):
    modelKM = KMeans(n_clusters=k, random_state=23)
    modelKM.fit(X_scaled)
    print('n_clusters=', k, ' : ', silhouette_score(X_scaled, modelKM.labels_))

# Détermination des centres des clusters
modelKM = KMeans(n_clusters = 2)
modelKM.fit(X_scaled)
centre = pd.DataFrame(modelKM.cluster_centers_)
centre

df['cluster'] = modelKM.labels_
df['cluster'].value_counts(normalize = True)

# Visualisation des clusters
fig, axes = plt.subplots(1,2, figsize = (13,5))
sns.scatterplot(ax = axes[0], data = df, x = df['tenure'], y = df['TotalCharges'], hue = 'cluster')
sns.scatterplot(ax = axes[0], data = centre, x = centre.iloc[:,4], y = centre.iloc[:,8], marker = 'X', color = 'green', s = 200)
axes[0].set_title('Clusters définis par K-means')

sns.scatterplot(ax = axes[1], data = df, x = df['tenure'], y = df['TotalCharges'], hue = 'Churn')
axes[1].set_title('Répartition en fonction des churns')

plt.show()

"""## Algorithmes de classification supervisé"""

# Définition de X et de y
X = df.drop(columns = ['customerID', 'Churn', 'cluster'])
y = df['Churn']

# Train test 
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=23, stratify=y)

# Standardisation
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Test des meilleurs paramètres sur la régression logistique
logreg = LogisticRegression(solver="liblinear", max_iter=1000)
params = {"C": np.arange(0.1,1,0.1), "penalty":["l1", "l2"]}
grid = GridSearchCV(logreg, params, scoring="roc_auc", n_jobs=-1, return_train_score=True)

# On entraîne le gridsearch uniquement sur le X_train
grid.fit(X_train_scaled, y_train)
print("best score:",grid.best_params_)
print("best parameters:",grid.best_score_)

# on regarde s'il y a overfiting
results = pd.DataFrame(grid.cv_results_)
results[["params","mean_train_score","mean_test_score","rank_test_score"]].sort_values(by="rank_test_score")

# Test des meilleurs paramètres sur l'arbre de décision
dt = DecisionTreeClassifier()
params = {'max_depth' : range(1,51),
        'min_samples_leaf' : range(1,16),
        'min_samples_split' : [2, 5, 7, 10, 15, 30]}
grid = GridSearchCV(dt, params, scoring="roc_auc", n_jobs=-1, return_train_score=True)

# on entraine le gridsearch
grid.fit(X_train_scaled, y_train)
print("best score:",grid.best_score_)
print("best parameters:",grid.best_params_)

# on regarde s'il y a overfiting
results = pd.DataFrame(grid.cv_results_)
results[["params","mean_train_score","mean_test_score","rank_test_score"]].sort_values(by="rank_test_score")

# Test des meilleurs paramètres sur le KNN
knn = KNeighborsClassifier()
params = {'n_neighbors' : range (1,51),
        'weights' : ['uniform', 'distance'],
        'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}
grid = GridSearchCV(knn, params, scoring="roc_auc", n_jobs=-1, return_train_score=True)

# on entraine le gridsearch
grid.fit(X_train_scaled, y_train)
print("best score:",grid.best_score_)
print("best parameters:",grid.best_params_)

# on regarde s'il y a overfiting
results = pd.DataFrame(grid.cv_results_)
results[["params","mean_train_score","mean_test_score","rank_test_score"]].sort_values(by="rank_test_score")

# Initialisation avec les paramètres optimaux 
# en mettant un  poids sur le Churn = Yes afin que tous les churns soient prédits comme tels
logreg_final = LogisticRegression(solver="liblinear", max_iter=1000, C=0.4, penalty="l1", class_weight = {'Yes' : 280})
logreg_final.fit(X_train_scaled, y_train)

# Teste sur le X_test 
y_pred = logreg_final.predict_proba(X_test_scaled)[:,1] 

# Score 
roc_auc_score(y_test, y_pred)

# Matrice de confusion
plot_confusion_matrix(logreg_final, X_test_scaled, y_test, normalize="true");

# Entraînement sur un maximum de données avec les X et les y connus
X_scaled = scaler.fit_transform(X)
logreg_final.fit(X_scaled, y);

# Prédiction sur toutes les données 
X['predict'] = logreg_final.predict(X_scaled)

y = pd.DataFrame(y)
X  = X.merge(y, how = 'inner', left_index = True, right_index = True)
X

# Probabilité de prédire que le client résilie son contrat
prob_predict = logreg_final.predict_proba(X_scaled)
print(prob_predict)
print(logreg_final.classes_)

X['prob_churn'] = prob_predict[:,1]
X = X.sort_values(by = 'prob_churn', ascending = False)
X

# Importance des colonnes
importance = logreg_final.coef_[0]

feature_names = X.drop(columns = ['predict', 'Churn', 'prob_churn'])
feature_names = list(feature_names.columns)
feature_importances = pd.DataFrame(importance, index = feature_names).reset_index().sort_values(by = 0, ascending = False)

plt.figure(figsize = (20,15))
sns.barplot(data = feature_importances, x = feature_importances[0], y = feature_importances['index']);

feature_importances

